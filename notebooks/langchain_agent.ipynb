{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the standard sqlite3 module with pysqlite3\n",
    "# for compatibility with Chroma\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import json\n",
    "import langchain\n",
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import StructuredTool\n",
    "from typing import Optional, Type\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint import MemorySaver  # an in-memory checkpointer\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# credential json not required if you are working within vertex AI workbench\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/LLM-agent-with-Gemini/fleet-anagram-244304-7dafcc771b2f.json\"\n",
    "\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") # only if you are using text embedding model from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(model=\"gemini-1.5-pro\") # alternative can be gemini-1.5-flash which is faster but less accurate, and also cheaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Chain / Runnable Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "\n",
    "\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "introduction_template = \"\"\"You are a helpful assistant that can help me to complete the following API payload:\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here are some examples of interactions you might have with me:\n",
    "\n",
    "Q: B16C, 1126911, FPP\n",
    "A: name:p.dsid,value, value:B16C, name:p.lot, value:1126911, name:p.pid, value:FPP\n",
    "\n",
    "Q: Y42M, 11952591, FQQP\n",
    "A: name:p.dsid, value:Y42M, name:p.lot, value:11952591, name:p.pid, value:FQQP\n",
    "\n",
    "Q: Y42M, 1252391, FPC\n",
    "A: name:p.dsid, value:Y42M, name:p.lot, value:1252391, name:p.pid, value:FPC\n",
    "\n",
    "Wrap the answer in a dictionary with the structure of the example above. The input will be a string with the format \"dsid, lot, pid\" \n",
    "and the output should be a dictionary with the keys \"name:p.dsid\", \"name:p.lot\", and \"name:p.pid\" with the corresponding values from the input string.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that can help me to complete the following API payload:\n",
      "\n",
      "Here are some examples of interactions you might have with me:\n",
      "\n",
      "Q: B16C, 1126911, FPP\n",
      "A: name:p.dsid,value, value:B16C, name:p.lot, value:1126911, name:p.pid, value:FPP\n",
      "\n",
      "Q: Y42M, 11952591, FQQP\n",
      "A: name:p.dsid, value:Y42M, name:p.lot, value:11952591, name:p.pid, value:FQQP\n",
      "\n",
      "Q: Y42M, 1252391, FPC\n",
      "A: name:p.dsid, value:Y42M, name:p.lot, value:1252391, name:p.pid, value:FPC\n",
      "\n",
      "Wrap the answer in a dictionary with the structure of the example above. The input will be a string with the format \"dsid, lot, pid\" \n",
      "and the output should be a dictionary with the keys \"name:p.dsid\", \"name:p.lot\", and \"name:p.pid\" with the corresponding values from the input string.\n",
      "\n",
      "\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: B16C, 1126911, FPP\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        input=\"B16C, 1126911, FPP\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"name:p.dsid\": \"B47R\",\n",
      "\"name:p.lot\": \"1952591\",\n",
      "\"name:p.pid\": \"FPC\"\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = pipeline_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({\"input\": \"B47R,1952591,FPC\"})\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def format_api_payload(input_string):\n",
      "  \"\"\"Formats the input string into an API payload dictionary.\n",
      "\n",
      "  Args:\n",
      "    input_string: A string containing dsid, lot, and pid separated by commas.\n",
      "\n",
      "  Returns:\n",
      "    A dictionary representing the API payload, or an error message if the input\n",
      "    is invalid.\n",
      "  \"\"\"\n",
      "  try:\n",
      "    dsid, lot, pid = input_string.split(',')\n",
      "    return {\n",
      "        \"name:p.dsid\": dsid.strip(),\n",
      "        \"name:p.lot\": lot.strip(),\n",
      "        \"name:p.pid\": pid.strip()\n",
      "    }\n",
      "  except ValueError:\n",
      "    return \"Invalid input format. Please use the format 'dsid, lot, pid'.\"\n",
      "\n",
      "# Example usage with your test case:\n",
      "input_string = \"test\"\n",
      "output_payload = format_api_payload(input_string)\n",
      "print(output_payload)  \n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Function Definition:**\n",
      "   - We define a function called `format_api_payload` that takes the input string as an argument.\n",
      "\n",
      "2. **Input Validation:**\n",
      "   - We use `try-except` to handle cases where the input string might not be in the expected \"dsid, lot, pid\" format. If the input is invalid, it returns an error message.\n",
      "\n",
      "3. **String Splitting and Stripping:**\n",
      "   - If the input is valid, we use `input_string.split(',')` to split the string into three parts based on the comma delimiter.\n",
      "   - `strip()` is applied to each part to remove any leading or trailing whitespace.\n",
      "\n",
      "4. **Dictionary Creation:**\n",
      "   - We create a dictionary with the keys `\"name:p.dsid\"`, `\"name:p.lot\"`, and `\"name:p.pid\"` and assign the corresponding values of `dsid`, `lot`, and `pid` extracted from the input string.\n",
      "\n",
      "5. **Returning the Payload:**\n",
      "   - The function returns the constructed dictionary, which represents the API payload.\n",
      "\n",
      "**Running the code with the input \"test\" will result in the following output:**\n",
      "\n",
      "```\n",
      "Invalid input format. Please use the format 'dsid, lot, pid'. \n",
      "```\n",
      "This is because the input \"test\" does not follow the required format.\n",
      "\n",
      "**Example with correct format:**\n",
      "\n",
      "If you input something like \"B16C, 1126911, FPP\", the output will be:\n",
      "\n",
      "```\n",
      "{'name:p.dsid': 'B16C', 'name:p.lot': '1126911', 'name:p.pid': 'FPP'}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"input\": \"test\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand! You want me to take an input like \"dsid, lot, pid\" and give you a dictionary like this:\n",
      "\n",
      "```python\n",
      "{\n",
      "  \"name:p.dsid\": \"dsid\",\n",
      "  \"name:p.lot\": \"lot\",\n",
      "  \"name:p.pid\": \"pid\"\n",
      "} \n",
      "```\n",
      "\n",
      "Let's forget about Langchain for now, I can definitely help you with that! Ask me your \"dsid, lot, pid\" string and I'll format it for your API payload. ðŸ˜Š  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"input\": \"What is langchain\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LLM as a knowledge base retriever\n",
    "\n",
    "web_paths = [\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://google.github.io/styleguide/Rguide.html\",\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for path in web_paths:\n",
    "    loader = WebBaseLoader(web_paths=(path,))\n",
    "    docs += loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.cache/pypoetry/virtualenvs/llm-agent-with-gemini-6S2l9oJE-py3.10/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/codespace/.cache/pypoetry/virtualenvs/llm-agent-with-gemini-6S2l9oJE-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=HuggingFaceEmbeddings()) # needs huggingface api key\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextualize or internalize the current input question with the chat history\n",
    "# This is useful when the current question is referencing the chat history\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7946035fd990>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "           | ChatVertexAI(client=<google.cloud.aiplatform_v1beta1.services.prediction_service.client.PredictionServiceClient object at 0x7946e2d663b0>, project='fleet-anagram-244304', model_name='gemini-1.5-pro', model_family=<GoogleModelFamily.GEMINI: '1'>, full_model_name='projects/fleet-anagram-244304/locations/us-central1/publishers/google/models/gemini-1.5-pro')\n",
      "           | StrOutputParser()\n",
      "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7946035fd990>)), config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "            | ChatVertexAI(client=<google.cloud.aiplatform_v1beta1.services.prediction_service.client.PredictionServiceClient object at 0x7946e2d663b0>, project='fleet-anagram-244304', model_name='gemini-1.5-pro', model_family=<GoogleModelFamily.GEMINI: '1'>, full_model_name='projects/fleet-anagram-244304/locations/us-central1/publishers/google/models/gemini-1.5-pro')\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Google Python Style Guide suggests using a consistent style for things like docstrings, comments, and variable names. Docstrings should use the three double-quotes format, and descriptiveness in naming should be proportional to the scope of visibility. Always strive for consistency with the existing codebase you are working with. \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are the best practices for Python code style?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"testing\"}\n",
    "    },  # constructs a key \"testing\" in `store` to store the chat history of the session\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericInputSchema(BaseModel):\n",
    "    \"\"\"Inputs for url navigation tool.\"\"\"\n",
    "    domain: str = Field(\n",
    "        description=\"The domain of the website you want to navigate to.\")\n",
    "\n",
    "class ParamInputSchema(BaseModel):\n",
    "    \"\"\"Inputs API payload.\"\"\"\n",
    "    params: str = Field(\n",
    "        description=\"The input string with the format 'dsid, lot, type', which can be used to create a dictionary with the keys 'name:p.dsid', 'name:p.lot', and 'name:p.type'.\")\n",
    "    \n",
    "class payload_formatter(BaseTool):\n",
    "    name: str = \"payload_formatter\"\n",
    "    args_schema: Optional[Type[BaseModel]] = ParamInputSchema\n",
    "    description: str = \"\"\"\n",
    "    \n",
    "    payload formatter is a tool that takes an input string with the format 'dsid, lot, type' and returns a dictionary with the keys 'name:p.dsid', 'name:p.lot', and 'name:p.type'.\n",
    "    \"\"\"\n",
    "    def format_input(self,input_string):\n",
    "        parts = input_string.replace('.', '').split(',')\n",
    "        parts = [part.strip() for part in parts if part.strip() != '']\n",
    "        if len(parts) != 3:\n",
    "            raise ValueError(\"Please check that the input contains exactly three parts: dsid, lot, and pid.\")\n",
    "        for part in parts:\n",
    "            if part[0].isalpha() and part[-1].isalpha() and part[1:-1].isdigit():\n",
    "                dsid = part\n",
    "            elif part.isdigit():\n",
    "                lot = part\n",
    "            else:\n",
    "                type_ = part\n",
    "        output = {\n",
    "            \"name:p.dsid\": dsid,\n",
    "            \"name:p.lot\": lot,\n",
    "            \"name:p.type\": type_\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def _run(self, params: str):\n",
    "        return self.format_input(params)\n",
    "    \n",
    "class url_navigator(BaseTool):\n",
    "    name: str = \"map_viewer_url\"\n",
    "    args_schema: Optional[Type[BaseModel]] = GenericInputSchema\n",
    "    description: str = \"\"\"\n",
    "    \n",
    "    url navigator is a tool that takes a domain as input and returns the url of the website you want to navigate to.\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self, domain: str):\n",
    "        return f\"www.{domain}.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [url_navigator(), payload_formatter()] # create a list of tools which will be the input of create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant and you know how to use url navigator tool and payload formatter tool. Pick the most appropriate tools to solve different tasks\"\n",
    "# not compulsory but I want to remind it the tools that it has access to\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = create_react_agent(\n",
    "    llm, tools, messages_modifier=system_message, checkpointer=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"name:p.dsid\": \"B47R\", \"name:p.lot\": \"1952591\", \"name:p.type\": \"FPC\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"chat-1\"}}\n",
    "\n",
    "print(\n",
    "    app.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", \"1952591,,,,,B47R,FPC\")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )[\"messages\"][-1].content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.langchain.com\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    app.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"Navigate to langchain\")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )[\"messages\"][-1].content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding in other built-in tools (e.g., search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults(max_results=2)\n",
    "\n",
    "tools = [url_navigator(), payload_formatter(), search]\n",
    "\n",
    "system_message = \"You are a helpful assistant and you know how to use url navigator tool, payload formatter tool and tavily search.\"\n",
    "# not compulsory but I want to remind it the tools that it has access to\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = create_react_agent(\n",
    "    llm, tools, messages_modifier=system_message, checkpointer=memory\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"test-thread\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    app.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"Search for the latest Conor McGregor news\")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )[\"messages\"][-1].content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "knowledge_retriever = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"Google style guide retriever\",\n",
    "    \"Searches and returns key pointers for programming languages from google style guide\",\n",
    ")\n",
    "tools = [url_navigator(), payload_formatter(), search, knowledge_retriever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant and you know how to use url navigator tool, payload formatter tool, tavily search and google style guide retriever.\"\n",
    "\n",
    "\n",
    "app = create_react_agent(\n",
    "    llm, tools, messages_modifier=system_message, checkpointer=memory\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"chat-thread\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\":\"Python rules\"}, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-with-gemini-6S2l9oJE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
