{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the standard sqlite3 module with pysqlite3\n",
    "# for compatibility with Chroma\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "\n",
    "import langchain\n",
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# credential json not required if you are working within vertex AI\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/LLM-agent-with-Gemini/fleet-anagram-244304-7dafcc771b2f.json\"\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") # only if you are using text embedding model from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Calculator\n",
      "multiply numbers\n",
      "{'a': {'title': 'A', 'description': 'first number', 'type': 'integer'}, 'b': {'title': 'B', 'description': 'second number', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: int = Field(description=\"first number\")\n",
    "    b: int = Field(description=\"second number\")\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "calculator = StructuredTool.from_function(\n",
    "    func=multiply,\n",
    "    name=\"Calculator\",\n",
    "    description=\"multiply numbers\",\n",
    "    args_schema=CalculatorInput,\n",
    "    return_direct=True,\n",
    "    # coroutine= ... <- you can specify an async method if desired as well\n",
    ")\n",
    "\n",
    "print(calculator.invoke({\"a\": 2, \"b\": 3}))\n",
    "print(calculator.name)\n",
    "print(calculator.description)\n",
    "print(calculator.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up_dictionary = {\n",
    "    \"Map viewer for detecting defects\":\"https://www.python.org/\",\n",
    "    \"time series trends\":\"http://python.langchain.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [lookup]\n",
    "llm = ChatVertexAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyworkdLookUp\n",
      "Infer the keywords from user input and use it to match against the keys that has the highest relevance in the dictionary and retrieve the URL associated\n",
      "{'k': {'title': 'K', 'description': 'key of the dictionary', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process\n",
    "class KeywordInput(BaseModel):\n",
    "    k: str = Field(description=\"key of the dictionary\")\n",
    "\n",
    "def keyword_to_url(k:str)->str:\n",
    "    best_match = process.extractOne(k, look_up_dictionary.keys())\n",
    "    if best_match:\n",
    "        return look_up_dictionary.get(best_match[0], \"URL not found\")\n",
    "    else:\n",
    "        return \"URL not found\"\n",
    "\n",
    "lookup = StructuredTool.from_function(\n",
    "    func=keyword_to_url,\n",
    "    name=\"KeyworkdLookUp\",\n",
    "    description=\"Infer the keywords from user input and use it to match against the keys that has the highest relevance in the dictionary and retrieve the URL associated\",\n",
    "    args_schema=KeywordInput,\n",
    "    return_direct=True,\n",
    ")\n",
    "\n",
    "\n",
    "# print(calculator.invoke({\"a\": 2, \"b\": 3}))\n",
    "print(lookup.name)\n",
    "print(lookup.description)\n",
    "print(lookup.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mQuestion: I want to view the defects of B16C\n",
      "Thought: I need to find the URL related to defects of B16C.\n",
      "Action: KeyworkdLookUp\n",
      "Action Input: defects of B16C\u001b[0m\u001b[36;1m\u001b[1;3mhttps://www.python.org/\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I want to view the defects of B16C',\n",
       " 'output': 'https://www.python.org/'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"I want to view the defects of B16C\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_payload = \"\"\"\n",
    "Task: Fill up the API payload based on the user input\n",
    "\n",
    "\n",
    "Payload Structure:\n",
    "{\n",
    "    \"dsid\": (string, examples: \"B47R\",\"B16C\",\"Y42M\"),\n",
    "    \"lot\": (int, examples: 1952291, 1126911,2023161),\n",
    "    \"pid\":(string, examples:\"FPP\",\"FQQP\",\"FPC\")\n",
    "}\n",
    "\n",
    "User Input:\n",
    "{user input}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Extract the dsid, lot, pid from the user input.\n",
    "2. Construct a valid API payload according to the provided structure\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: B47R, 1952291, FQQP\n",
      "Thought: I need more information to understand the pattern in the given sequence. There's a combination of letters and numbers, and it's unclear how they relate. \n",
      "Final Answer: I cannot answer this question. I need more context or information about the sequence to identify a pattern and provide the next element. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'B47R, 1952291, FQQP',\n",
       " 'output': 'I cannot answer this question. I need more context or information about the sequence to identify a pattern and provide the next element.'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = []\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"B47R, 1952291, FQQP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='I want to view the wafer map, use lookup', id='caf9d967-3a87-4671-8fbe-1fa4742cf2e5'),\n",
       " AIMessage(content='Could you please provide the keyword for the wafer map? \\n', response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'usage_metadata': {'prompt_token_count': 44, 'candidates_token_count': 13, 'total_token_count': 57}}, id='run-348342eb-6056-40d6-86a7-e799c113715b-0')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent_executor = chat_agent_executor.create_tool_calling_executor(llm, tools)\n",
    "\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"I want to view the wafer map, use lookup\")]}\n",
    ")\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults(max_results=3)\n",
    "result = search.invoke(\"What is Julia language known for?\")\n",
    "\n",
    "# Formatting\n",
    "for i, item in enumerate(result, start=1):\n",
    "    print(f\"{i}. URL: {item['url']}\\nContent: {item['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_paths = [\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://google.github.io/styleguide/Rguide.html\",\n",
    "    # \"https://google.github.io/styleguide/cppguide.html\",\n",
    "    # \"https://google.github.io/styleguide/go/\",\n",
    "    # \"https://google.github.io/styleguide/javaguide.html\",\n",
    "    # \"https://google.github.io/styleguide/jsguide.html\",\n",
    "    # \"https://google.github.io/styleguide/tsguide.html\"\n",
    "]\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "# bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "docs = []\n",
    "for path in web_paths:\n",
    "    loader = WebBaseLoader(web_paths=(path,),\n",
    "                        #    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    "    )\n",
    "    docs += loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"programming_style_guide_search\",\n",
    "    \"Reference for information about styling rules for programming languages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = chat_agent_executor.create_tool_calling_executor(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            print(f\"Question: {response['messages'][0].content}\")\n",
    "        elif i == 1:\n",
    "            function_call = response['messages'][i].additional_kwargs.get('function_call', {})\n",
    "            name = function_call.get('name', '')\n",
    "            print(f\"Tool: {name}\")\n",
    "        elif i == 2:\n",
    "            data = json.loads(response['messages'][2].content)\n",
    "            print(\"Citation:\")\n",
    "            for item in data:\n",
    "                print(item['url'])\n",
    "        else:\n",
    "            print(f\"Answer: {response['messages'][3].content}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What are the differences between langchain and llamaindex, search if it's not in retriever\")]}\n",
    ")\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamin\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is Vajra\")]}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = chat_agent_executor.create_tool_calling_executor(\n",
    "    llm, tools, checkpointer=memory\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I'm Hanyu\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Suggest some use cases for LLM Agent\")]}, config\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-with-gemini-6S2l9oJE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
